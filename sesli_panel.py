import streamlit as st
from groq import Groq
from gtts import gTTS
from io import BytesIO
from streamlit_mic_recorder import mic_recorder
import base64

# --- SAYFA AYARLARI ---
st.set_page_config(page_title="SEAS V2 - Vision & Voice", layout="centered")

# API BaÄŸlantÄ±sÄ±
client = Groq(api_key=st.secrets["GROQ_API_KEY"])

if "messages" not in st.session_state:
    st.session_state.messages = []

st.title("ğŸ™ï¸ğŸ‘ï¸ SEAS V2: Sesli & GÃ¶rsel")

# --- YAN PANEL: GÃ–RSEL YÃœKLEME ---
with st.sidebar:
    st.header("ğŸ–¼ï¸ Soru/GÃ¶rsel YÃ¼kle")
    uploaded_file = st.file_uploader("Bir resim seÃ§ veya Ã§ek...", type=['png', 'jpg', 'jpeg'])
    if uploaded_file:
        st.image(uploaded_file, caption="YÃ¼klenen GÃ¶rsel", use_container_width=True)

# --- SES KAYIT BÃ–LÃœMÃœ ---
audio_input = mic_recorder(
    start_prompt="ğŸ¤ KonuÅŸarak Soru Sor",
    stop_prompt="ğŸ›‘ Bitirmek Ä°Ã§in Bas",
    key='mic'
)

# --- SOHBET AKIÅI ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- YARDIMCI FONKSÄ°YON: RESMÄ° OKUMA ---
def encode_image(image_file):
    return base64.b64encode(image_file.read()).decode('utf-8')

# --- Ä°ÅLEME ---
prompt = ""
if audio_input:
    audio_bio = BytesIO(audio_input['bytes'])
    audio_bio.name = "audio.wav"
    transcription = client.audio.transcriptions.create(
        file=audio_bio,
        model="whisper-large-v3",
        language="tr"
    )
    prompt = transcription.text

text_input = st.chat_input("Veya buraya yaz...")
if text_input: prompt = text_input

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # EÄŸer resim varsa Vision modelini, yoksa normal modeli kullanÄ±yoruz
        if uploaded_file:
            base64_image = encode_image(uploaded_file)
            # Resim varken Llama 3.2 Vision modelini kullanÄ±yoruz
            response = client.chat.completions.create(
                model="llama-3.2-90b-vision-preview",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": f"GÃ¶rseli analiz et ve ÅŸu soruya cevap ver: {prompt}"},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                        ]
                    }
                ]
            )
        else:
            # Resim yoksa standart hÄ±zlÄ± modeli kullanÄ±yoruz
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "system", "content": "Sen samimi bir kankasÄ±n. SorularÄ± Ã§Ã¶zerken adÄ±m adÄ±m aÃ§Ä±kla."},
                          {"role": "user", "content": prompt}]
            )
        
        cevap = response.choices[0].message.content
        st.markdown(cevap)
        st.session_state.messages.append({"role": "assistant", "content": cevap})
        
        # SESLENDÄ°RME
        tts = gTTS(text=cevap, lang='tr')
        audio_fp = BytesIO()
        tts.write_to_fp(audio_fp)
        st.audio(audio_fp, format='audio/mp3', autoplay=True)
